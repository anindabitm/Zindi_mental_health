{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Zindi-mental-health-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anindabitm/Zindi_mental_health/blob/master/Zindi_mental_health_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPoTMzzPzVVq",
        "colab_type": "text"
      },
      "source": [
        "This notebook is based on the wonderful notebook and video made by Abhishek Thakur and the NLP Albumenations idea in this kernel\n",
        "https://www.kaggle.com/shonenkov/nlp-albumentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kvYAbb6FzVVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mjpRA5rKzVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install textblob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "a_jLHIkwzVV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "1JDrbaT-zVV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob \n",
        "import random\n",
        "from nltk import sent_tokenize\n",
        "from tqdm import tqdm\n",
        "from albumentations.core.transforms_interface import DualTransform, BasicTransform\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import contractions\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AlPvT3HOzVWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLPTransform(BasicTransform):\n",
        "    \"\"\" Transform for nlp task.\"\"\"\n",
        "    \n",
        "    @property\n",
        "    def targets(self):\n",
        "        return {\"data\": self.apply}\n",
        "    \n",
        "    def update_params(self, params, **kwargs):\n",
        "        if hasattr(self, \"interpolation\"):\n",
        "            params[\"interpolation\"] = self.interpolation\n",
        "        if hasattr(self, \"fill_value\"):\n",
        "            params[\"fill_value\"] = self.fill_value\n",
        "        return params\n",
        "\n",
        "    def get_sentences(self, text):\n",
        "        return sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RvrtuyyTzVWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShuffleSentencesTransform(NLPTransform):\n",
        "    \"\"\" Do shuffle by sentence \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text = data\n",
        "        sentences = self.get_sentences(text)\n",
        "        random.shuffle(sentences)\n",
        "        return ' '.join(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S8fTBU7tzVWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExcludeDuplicateSentencesTransform(NLPTransform):\n",
        "    \"\"\" Exclude equal sentences \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeDuplicateSentencesTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text = data\n",
        "        sentences = []\n",
        "        for sentence in self.get_sentences(text):\n",
        "            sentence = sentence.strip()\n",
        "            if sentence not in sentences:\n",
        "                sentences.append(sentence)\n",
        "        return ' '.join(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "a7cLR7v4zVWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExcludeNumbersTransform(NLPTransform):\n",
        "    \"\"\" exclude any numbers \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text = data\n",
        "        text = re.sub(r'[0-9]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rLyNjvQwzVWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExcludeHashtagsTransform(NLPTransform):\n",
        "    \"\"\" Exclude any hashtags with # \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text = data\n",
        "        text = re.sub(r'#[\\S]+\\b', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PevsoGKGzVWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExcludeUsersMentionedTransform(NLPTransform):\n",
        "    \"\"\" Exclude @users \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text = data\n",
        "        text = re.sub(r'@[\\S]+\\b', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UkJmNGI2zVWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExcludeUrlsTransform(NLPTransform):\n",
        "    \"\"\" Exclude urls \"\"\"\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text = data\n",
        "        text = re.sub(r'https?\\S+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VBLZBqfdzVWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SwapWordsTransform(NLPTransform):\n",
        "    \"\"\" Swap words next to each other \"\"\"\n",
        "    def __init__(self, swap_distance=1, swap_probability=0.1, always_apply=False, p=0.5):\n",
        "        \"\"\"  \n",
        "        swap_distance - distance for swapping words\n",
        "        swap_probability - probability of swapping for one word\n",
        "        \"\"\"\n",
        "        super(SwapWordsTransform, self).__init__(always_apply, p)\n",
        "        self.swap_distance = swap_distance\n",
        "        self.swap_probability = swap_probability\n",
        "        self.swap_range_list = list(range(1, swap_distance+1))\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text = data\n",
        "        words = text.split()\n",
        "        words_count = len(words)\n",
        "        if words_count <= 1:\n",
        "            return text\n",
        "\n",
        "        new_words = {}\n",
        "        for i in range(words_count):\n",
        "            if random.random() > self.swap_probability:\n",
        "                new_words[i] = words[i]\n",
        "                continue\n",
        "    \n",
        "            if i < self.swap_distance:\n",
        "                new_words[i] = words[i]\n",
        "                continue\n",
        "    \n",
        "            swap_idx = i - random.choice(self.swap_range_list)\n",
        "            new_words[i] = new_words[swap_idx]\n",
        "            new_words[swap_idx] = words[i]\n",
        "\n",
        "        return ' '.join([v for k, v in sorted(new_words.items(), key=lambda x: x[0])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "z05mwCA8zVWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CutOutWordsTransform(NLPTransform):\n",
        "    \"\"\" Remove random words \"\"\"\n",
        "    def __init__(self, cutout_probability=0.05, always_apply=False, p=0.5):\n",
        "        super(CutOutWordsTransform, self).__init__(always_apply, p)\n",
        "        self.cutout_probability = cutout_probability\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text = data\n",
        "        words = text.split()\n",
        "        words_count = len(words)\n",
        "        if words_count <= 1:\n",
        "            return text\n",
        "        \n",
        "        new_words = []\n",
        "        for i in range(words_count):\n",
        "            if random.random() < self.cutout_probability:\n",
        "                continue\n",
        "            new_words.append(words[i])\n",
        "\n",
        "        if len(new_words) == 0:\n",
        "            return words[random.randint(0, words_count-1)]\n",
        "\n",
        "        return ' '.join(new_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QEVCn_X1zVWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import albumentations\n",
        "\n",
        "def get_train_transforms():\n",
        "    return albumentations.Compose([\n",
        "        ExcludeDuplicateSentencesTransform(p=0.5),  # here not p=1.0 because your nets should get some difficulties\n",
        "        ShuffleSentencesTransform(p=0.5),\n",
        "        ExcludeNumbersTransform(p=0.5),\n",
        "        ExcludeHashtagsTransform(p=0.5),\n",
        "        ExcludeUsersMentionedTransform(p=0.5),\n",
        "        ExcludeUrlsTransform(p=0.5),\n",
        "        CutOutWordsTransform(p=0.5),\n",
        "        SwapWordsTransform(p=0.5),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uaiJWTb_zVWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = contractions.fix(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"\n",
        "    Removing stopwords belonging to english language\n",
        "    \n",
        "    \"\"\"\n",
        "    words = [w for w in text if w not in stopwords.words('english')]\n",
        "    return words\n",
        "\n",
        "def combine_text(list_of_text):\n",
        "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
        "    combined_text = ' '.join(list_of_text)\n",
        "    return combined_text\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    Cleaning and parsing the text.\n",
        "\n",
        "    \"\"\"\n",
        "    #tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "    nopunc = clean_text(text)\n",
        "    #tokenized_text = tokenizer.tokenize(nopunc)\n",
        "    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
        "    #combined_text = ' '.join(remove_stopwords)\n",
        "    return nopunc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5rmaWRD9zVWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('/kaggle/input/zindimentalhealth/Train.csv')\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gWPB23RwzVW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VqQyEhX2zVXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_text = df.text.values\n",
        "len(all_text)\n",
        "for idx in range(len(all_text)):\n",
        "    all_text[idx] = str(TextBlob(all_text[idx]).correct())\n",
        "\n",
        "all_text[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y_QPul-mzVXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text']=all_text\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HTyiCM1VzVXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text'] = df['text'].apply(lambda x : text_preprocessing(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WRrYajKYzVXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['label'])\n",
        "df.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7_Fya8K9zVXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WIWSuUDszVXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqY_XVCBzVXQ",
        "colab_type": "text"
      },
      "source": [
        "# Lets try BERT !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "itgqN9gvzVXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Configuration\n",
        "\n",
        "import transformers\n",
        "import tokenizers\n",
        "\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 5\n",
        "#BERT_PATH = \"/kaggle/input/bert-pytorch/\"\n",
        "#BERT_PATH = \"/kaggle/input/bert-base-uncased/\"\n",
        "#MODEL_PATH = \"bert-large-uncased-pytorch_model.bin\"\n",
        "#TRAINING_FILE = \"../input/imdb.csv\"\n",
        "TOKENIZER_BERT = transformers.BertTokenizer.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    do_lower_case=True\n",
        ")\n",
        "\n",
        "TOKENIZER_ROBERTA = transformers.RobertaTokenizer.from_pretrained(\n",
        "    'roberta-base',\n",
        "    lowercase=True,\n",
        "    add_prefix_space=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IREcXtOfzVXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BERTDataset:\n",
        "    def __init__(self,text,label,train_transforms=None):\n",
        "        self.text = text\n",
        "        self.label=label\n",
        "        self.tokenizer = TOKENIZER_BERT\n",
        "        self.max_len = MAX_LEN\n",
        "        self.train_transforms = train_transforms\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.text[item])\n",
        "        text = \" \".join(text.split())\n",
        "        if self.train_transforms:\n",
        "            text = self.train_transforms(data=text)['data']\n",
        "        \n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'label': torch.tensor(self.label[item], dtype=torch.long),\n",
        "                       \n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b3pn-qstzVXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ROBERTADataset:\n",
        "    def __init__(self,text,label,train_transforms=None):\n",
        "        self.text = text\n",
        "        self.label=label\n",
        "        self.tokenizer = TOKENIZER_ROBERTA\n",
        "        self.max_len = MAX_LEN\n",
        "        self.train_transforms = train_transforms\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.text[item])\n",
        "        text = \" \".join(text.split())\n",
        "        if self.train_transforms:\n",
        "            text = self.train_transforms(data=text)['data']\n",
        "        \n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        \n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'label': torch.tensor(self.label[item], dtype=torch.long),\n",
        "                       \n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "N5-O-IduzVXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "def linear_combination(x, y, epsilon): \n",
        "    return epsilon*x + (1-epsilon)*y\n",
        "\n",
        "def reduce_loss(loss, reduction='mean'):\n",
        "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
        "\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, epsilon:float=0.05, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.reduction = reduction\n",
        "    \n",
        "    def forward(self, preds, target):\n",
        "        n = preds.size()[-1]\n",
        "        log_preds = F.log_softmax(preds, dim=-1)\n",
        "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
        "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
        "        return linear_combination(loss/n, nll, self.epsilon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nmed83YQzVXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device=torch.device('cuda')\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "#loss_fn = LabelSmoothingCrossEntropy().to(device)\n",
        "\n",
        "\n",
        "\n",
        "def train_bert(data_loader, model, optimizer, device, scheduler,n_example):\n",
        "    model.train()\n",
        "    losses=[]\n",
        "    correct_predictions=0\n",
        "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        ids = d[\"ids\"]\n",
        "        token_type_ids = d[\"token_type_ids\"]\n",
        "        mask = d[\"mask\"]\n",
        "        label = d[\"label\"]\n",
        "        \n",
        "        \n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        label = label.to(device, dtype=torch.long)\n",
        "        \n",
        "        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_fn(outputs, label)\n",
        "        correct_predictions += torch.sum(preds == label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "    return correct_predictions.double()/n_example,np.mean(losses)\n",
        "\n",
        "                      \n",
        "\n",
        "\n",
        "def eval_bert(data_loader, model, device,n_example):\n",
        "    model.eval()\n",
        "    losses=[]\n",
        "    correct_predictions=0\n",
        "    with torch.no_grad():\n",
        "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            label = d[\"label\"]\n",
        "            \n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "            \n",
        "\n",
        "            outputs = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            \n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, label)\n",
        "            correct_predictions += torch.sum(preds == label)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "    return correct_predictions.double()/n_example,np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q7KWeRAOzVXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_roberta(data_loader, model, optimizer, device, scheduler,n_example):\n",
        "    model.train()\n",
        "    losses=[]\n",
        "    correct_predictions=0\n",
        "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "        ids = d[\"ids\"]\n",
        "        \n",
        "        mask = d[\"mask\"]\n",
        "        label = d[\"label\"]\n",
        "        \n",
        "        \n",
        "        ids = ids.to(device, dtype=torch.long)\n",
        "        \n",
        "        mask = mask.to(device, dtype=torch.long)\n",
        "        label = label.to(device, dtype=torch.long)\n",
        "        \n",
        "        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            ids=ids,\n",
        "            mask=mask,\n",
        "            \n",
        "        )\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_fn(outputs, label)\n",
        "        correct_predictions += torch.sum(preds == label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "    return correct_predictions.double()/n_example,np.mean(losses)\n",
        "\n",
        "                      \n",
        "\n",
        "\n",
        "def eval_roberta(data_loader, model, device,n_example):\n",
        "    model.eval()\n",
        "    losses=[]\n",
        "    correct_predictions=0\n",
        "    with torch.no_grad():\n",
        "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            ids = d[\"ids\"]\n",
        "            \n",
        "            mask = d[\"mask\"]\n",
        "            label = d[\"label\"]\n",
        "            \n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            \n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "            \n",
        "\n",
        "            outputs = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                \n",
        "            )\n",
        "            \n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, label)\n",
        "            correct_predictions += torch.sum(preds == label)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "    return correct_predictions.double()/n_example,np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "N3GjJVCPzVXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTBaseUncased, self).__init__()\n",
        "        #self.bert = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, 4)\n",
        "    \n",
        "    def forward(self, ids, mask,token_type_ids):\n",
        "        _, o2 = self.bert(\n",
        "            ids, \n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        bo = self.bert_drop(o2)\n",
        "        output = self.out(bo)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o8QC6Fo8zVXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ROBERTABase(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ROBERTABase, self).__init__()\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "        self.bert_drop = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(768, 4)\n",
        "    \n",
        "    def forward(self, ids, mask):\n",
        "        _, o2 = self.roberta(\n",
        "            ids, \n",
        "            attention_mask=mask,\n",
        "            )\n",
        "        bo = self.bert_drop(o2)\n",
        "        output = self.out(bo)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ewBRiSSizVXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "df_test=pd.read_csv('/kaggle/input/zindimentalhealth/Test.csv')\n",
        "df_test['label'] = 0\n",
        "all_text = df_test.text.values\n",
        "len(all_text)\n",
        "for idx in range(len(all_text)):\n",
        "    all_text[idx] = str(TextBlob(all_text[idx]).correct())\n",
        "df_test['text'] = all_text\n",
        "df_test['text'] = df_test['text'].apply(lambda x : text_preprocessing(x))\n",
        "\n",
        "\n",
        "test_dataset = BERTDataset(\n",
        "        text=df_test.text.values,\n",
        "        label=df_test.label.values,\n",
        "        train_transforms=None\n",
        "    )\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "    \n",
        "\n",
        "X = df['text']\n",
        "y = df['label']\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "fold = 0\n",
        "predictions_bert=np.zeros(shape=(309,4))\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    print('In fold #',fold+1)\n",
        "    print(\"#\"*80)\n",
        "    df_train = df.iloc[train_index,:]\n",
        "    df_valid = df.iloc[test_index,:]\n",
        "    df_train = df_train.reset_index(drop=True)\n",
        "    df_valid = df_valid.reset_index(drop=True)\n",
        "\n",
        "    train_dataset = BERTDataset(\n",
        "        text=df_train.text.values,\n",
        "        label=df_train.label.values,\n",
        "        train_transforms=get_train_transforms()\n",
        "        )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=TRAIN_BATCH_SIZE,\n",
        "        num_workers=4\n",
        "        )\n",
        "\n",
        "    valid_dataset = BERTDataset(\n",
        "        text=df_valid.text.values,\n",
        "        label=df_valid.label.values,\n",
        "        train_transforms=None\n",
        "        )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "        )\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = BERTBaseUncased()\n",
        "    model.to(device)\n",
        "    \n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "        ]\n",
        "\n",
        "    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "                    optimizer,\n",
        "                    num_warmup_steps=0,\n",
        "                    num_training_steps=num_train_steps\n",
        "                    )\n",
        "\n",
        "    #model = nn.DataParallel(model)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    logloss = 1.0\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "        print('-' * 70)\n",
        "        train_acc, train_loss = train_bert(train_data_loader, model, optimizer, device, scheduler,len(df_train))\n",
        "        print(f'Train accuracy {train_acc} & Training loss {train_loss}')\n",
        "        val_acc,val_loss = eval_bert(valid_data_loader, model, device,len(df_valid))\n",
        "        print(f'Validation accuracy {val_acc} & Validation loss {val_loss}')\n",
        "        if val_acc > best_accuracy:\n",
        "            torch.save(model.state_dict(), 'pytorch_model_bert.bin')\n",
        "            best_accuracy = val_acc\n",
        "            print('Model saved!!!')\n",
        "    \n",
        "    model = BERTBaseUncased()\n",
        "    model.load_state_dict(torch.load('pytorch_model_bert.bin'))\n",
        "    model = model.to(device)\n",
        "    model = model.eval()\n",
        "    prediction_probs = []\n",
        "    with torch.no_grad():\n",
        "        for bi, d in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
        "            ids = d[\"ids\"]\n",
        "            token_type_ids = d[\"token_type_ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            label = d[\"label\"]\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "            outputs = model(ids=ids,\n",
        "                            mask=mask,\n",
        "                            token_type_ids=token_type_ids\n",
        "                           )\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            prediction_probs.extend(probs)\n",
        "        \n",
        "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n",
        "    predictions_bert += prediction_probs\n",
        "    del model\n",
        "    gc.collect()\n",
        "    fold += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eAFtJpNbzVXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = ROBERTADataset(\n",
        "        text=df_test.text.values,\n",
        "        label=df_test.label.values,\n",
        "        train_transforms=None\n",
        "    )\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "    )\n",
        "    \n",
        "\n",
        "X = df['text']\n",
        "y = df['label']\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "fold = 0\n",
        "predictions_roberta=np.zeros(shape=(309,4))\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    print('In fold #',fold+1)\n",
        "    print(\"#\"*80)\n",
        "    df_train = df.iloc[train_index,:]\n",
        "    df_valid = df.iloc[test_index,:]\n",
        "    df_train = df_train.reset_index(drop=True)\n",
        "    df_valid = df_valid.reset_index(drop=True)\n",
        "\n",
        "    train_dataset = ROBERTADataset(\n",
        "        text=df_train.text.values,\n",
        "        label=df_train.label.values,\n",
        "        train_transforms=get_train_transforms()\n",
        "        )\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=TRAIN_BATCH_SIZE,\n",
        "        num_workers=4\n",
        "        )\n",
        "\n",
        "    valid_dataset = ROBERTADataset(\n",
        "        text=df_valid.text.values,\n",
        "        label=df_valid.label.values,\n",
        "        train_transforms=None\n",
        "        )\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=VALID_BATCH_SIZE,\n",
        "        num_workers=1\n",
        "        )\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = ROBERTABase()\n",
        "    model.to(device)\n",
        "    \n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "        ]\n",
        "\n",
        "    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "                    optimizer,\n",
        "                    num_warmup_steps=0,\n",
        "                    num_training_steps=num_train_steps\n",
        "                    )\n",
        "\n",
        "    #model = nn.DataParallel(model)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    logloss = 1.0\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "        print('-' * 70)\n",
        "        train_acc, train_loss = train_roberta(train_data_loader, model, optimizer, device, scheduler,len(df_train))\n",
        "        print(f'Train accuracy {train_acc} & Training loss {train_loss}')\n",
        "        val_acc,val_loss = eval_roberta(valid_data_loader, model, device,len(df_valid))\n",
        "        print(f'Validation accuracy {val_acc} & Validation loss {val_loss}')\n",
        "        if val_acc > best_accuracy:\n",
        "            torch.save(model.state_dict(), 'pytorch_model_roberta.bin')\n",
        "            best_accuracy = val_acc\n",
        "            print('Model saved!!!')\n",
        "    \n",
        "    model = ROBERTABase()\n",
        "    model.load_state_dict(torch.load('pytorch_model_roberta.bin'))\n",
        "    model = model.to(device)\n",
        "    model = model.eval()\n",
        "    prediction_probs = []\n",
        "    with torch.no_grad():\n",
        "        for bi, d in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
        "            ids = d[\"ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            label = d[\"label\"]\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            \n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "            outputs = model(ids=ids,\n",
        "                            mask=mask,\n",
        "                            \n",
        "                           )\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            prediction_probs.extend(probs)\n",
        "        \n",
        "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n",
        "    predictions_roberta += prediction_probs\n",
        "    del model\n",
        "    gc.collect()\n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZCSGIDYnzVXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = ((predictions_bert/10)*0.30 +(predictions_roberta/10)*0.70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FKe5WIkIzVXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ss=pd.read_csv('/kaggle/input/zindimentalhealth/SampleSubmission.csv')\n",
        "ss.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zV3ZeuuPzVX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ss.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Y7_pOP6PzVX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ss['Alcohol']=predictions[:,0]\n",
        "ss['Depression']=predictions[:,1]\n",
        "ss['Drugs']=predictions[:,2]\n",
        "ss['Suicide']=predictions[:,3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L6Swf3_AzVYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ss.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Q_VKHE4-zVYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ss.to_csv('my_sub_roberta_bert_ensmbl.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}